# 自然语言处理入门

## 自然语言NLP全景图

![https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-09-19-nlpct.png](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-09-19-nlpct.png)

## 自然语言处理

处理两个任务：

1. ⾃然语⾔理解 - NLU
    1. 难点:
        1. 语⾔的多样性
        2. 语⾔的歧义性
        3. 语⾔的鲁棒性
        4. 语⾔的知识依赖
        5. 语⾔的上下⽂
2. ⾃然语⾔⽣成 - NLG
    1. 步骤
        1. 内容确定 - Content Determination
        2. ⽂本结构 - Text Structuring
        3. 句⼦聚合 - Sentence Aggregation
        4. 语法化 - Lexicalisation
        5. 参考表达式⽣成 - Referring Expression Generation|REG
        6. 语⾔实现 - Linguistic Realisation
    

**传统机器学习的NLP流程**

1. 语料预处理
    1. 中⽂语料预处理 4 个步骤
        1. 中⽂分词 - Chinese Word Segmentation
        2. 词性标注 - Parts of Speech
        3. 命名实体识别 - NER
        4. 去除停⽤词
    2. 英⽂语料预处理的 6 个步骤
        1. 分词 - Tokenization
        2. 词⼲提取 - Stemming
        3. 词形还原 - Lemmatization
        4. 词性标注 - Parts of Speech
        5. 命名实体识别 - NER
        6. 分块 - Chunking
2. 特征⼯程
    1. 特征提取
    2. 特征选择
3. 选择分类器

**深度学习的NLP流程**

1. 语料预处理
    1. 中⽂语料预处理 4 个步骤
    2. 英⽂语料预处理的 6 个步骤
2. 设计模型
3. 模型训练

## 自然语言理解

关键技术点: **意图识别和实体提取**

**Transformer介绍**

[BERT大火却不懂Transformer？读这一篇就够了](https://zhuanlan.zhihu.com/p/54356280)

综合资料:

[放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)

[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

[效果惊人的GPT 2.0模型：它告诉了我们什么](https://zhuanlan.zhihu.com/p/56865533)

## 自然语言生成

⾃然语⾔⽣成 - NLG 有2种⽅式：

1. text - to - text：⽂本到语⾔的⽣成
2. data - to - text ：数据到语⾔的⽣成

NLG的3个LEVEL：

1. **简单的数据合并：**⾃然语⾔处理的简化形式，这将允许将数据转换为⽂本（通过类似Excel的函数）。为了关联，以邮件合并（MS Word mailmerge）为例，其中间隙填充了⼀些数据，这些数据是从另⼀个源（例如MS Excel中的表格）中检索的.
2. **模板化的 NLG** ：这种形式的NLG使⽤模板驱动模式来显示输出。以⾜球⽐赛得分板为例。数据动态地保持更改，并由预定义的业务规则集（如if / else循环语句）⽣成。
3. **⾼级 NLG** ：这种形式的⾃然语⾔⽣成就像⼈类⼀样。它理解意图，添加智能，考虑上下⽂，并将结果呈现在⽤户可以轻松阅读和理解的富有洞察⼒的叙述中

NLG的6个步骤:

1. 内容确定
    1. NLG 系统需要决定哪些信息应该包含在正在构建的⽂本中，哪些不应该包含。通常数据中包含的信息⽐最终传达的信息要多。
2. 文本结构
    1. 确定需要传达哪些信息后，NLG 系统需要合理的组织⽂本的顺序。例如在报道⼀场篮球⽐赛时，会优先表达「什么时间」「什么地点」「哪2⽀球队」，然后再表达「⽐赛的概况」，最后表达「⽐赛的结局」。
3. 句子聚合
    1. 不是每⼀条信息都需要⼀个独⽴的句⼦来表达，将多个信息合并到⼀个句⼦⾥表达可能会
    更加流畅，也更易于阅读
4. 语法化
    1. 当每⼀句的内容确定下来后，就可以将这些信息组织成⾃然语⾔了。这个步骤会在各种信息之间加⼀些连接词，看起来更像是⼀个完整的句⼦。
5. 参考表达式生成
    1. 这个步骤跟语法化很相似，都是选择⼀些单词和短语来构成⼀个完整的句⼦。不过他跟语法化的本质区别在于“REG需要识别出内容的领域，然后使⽤该领域（⽽不是其他领域）的词汇”。
6. 语言实现
    1. 最后，当所有相关的单词和短语都已经确定时，需要将它们组合起来形成⼀个结构良好的完整句⼦。

## 分词

**分词**: 分词就是将句⼦、段落、⽂章这种⻓⽂本，分解为以字词为单位的数据结构，⽅便后续的处理分析⼯作。

### **为什么要分词?**

1、**机器学习之所以看上去可以解决很多复杂的问题，是因为它把这些问题都转化为了数学问题**,⽽ NLP 也是相同的思路，⽂本都是⼀些「⾮结构化数据」，我们需要先将这些数据转化为「结构化数据」，结构化数据就可以转化为数学问题了，⽽分词就是转化的第⼀步。

2、**词是一个比较合适的粒度**，字的粒度太⼩，⽆法表达完整含义，⽐如”⿏“可以是”⽼⿏“，也可以是”⿏标“。⽽句⼦的粒度太⼤，承载的信息量多，很难复⽤。⽐如”传统⽅法要分词，⼀个重要原因
是传统⽅法对远距离依赖的建模能⼒较弱。”

3、**深度学习时代，部分任务中也可以「分字」**

### 中英文分词的典型区别

1、**分词⽅式不同，中⽂更难**

英⽂有天然的空格作为分隔符，但是中⽂没有。所以如何切分是⼀个难点，再加上中⽂⾥
⼀词多意的情况⾮常多，导致很容易出现歧义。

2、**英⽂单词有多种形态**

英⽂单词存在丰富的变形变换。为了应对这些复杂的变换，英⽂NLP相⽐中⽂存在⼀些独特的处理步骤，我们称为词形还原（Lemmatization）和词⼲提取（Stemming）。中⽂则不需要。

词性还原：does，done，doing，did 需要通过词性还原恢复成 do

词⼲提取：cities，children，teeth 这些词，需要转换为 city，child，tooth”这些基本
形态。

3、**中⽂分词需要考虑粒度问题**

例如「中国科学技术⼤学」就有很多种分法：
中国科学技术⼤学
中国 \ 科学技术 \ ⼤学
中国 \ 科学 \ 技术 \ ⼤学
粒度越⼤，表达的意思就越准确，但是也会导致召回⽐较少。所以中⽂需要不同的场景和
要求选择不同的粒度。这个在英⽂中是没有的

### 中文分词的难点

1、**没有统⼀的标准**

⽬前中⽂分词没有统⼀的标准，也没有公认的规范。不同的公司和组织各有各的⽅法和规
则。

2、**歧义词如何切分**

例如「兵乓球拍卖完了」就有2种分词⽅式表达了2种不同的含义：
乒乓球 \ 拍卖 \ 完了
乒乓 \ 球拍 \ 卖 \ 完了

3、**新词的识别**

信息爆炸的时代，三天两头就会冒出来⼀堆新词，如何快速的识别出这些新词是⼀⼤难点。⽐如当年「蓝瘦⾹菇」⼤⽕，就需要快速识别。

### 中文3种典型的分词方法

1、基于**词典匹配的分词⽅式**

将待分词的中⽂⽂本根据⼀定规则切分和调整，然后跟词典中的词语进⾏匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表⽅法有基于正向最⼤匹配和基于逆向最⼤匹配及双向匹配法

- 优点: 速度快、成本低
- 缺点: 适应性不强，不同领域效果差异⼤

2、基于统计的分词方式

这类⽬前常⽤的是算法是**HMM、CRF、SVM、深度学习**等算法，⽐如stanford、Hanlp分词⼯具是基于CRF算法。以CRF为例，基本思路是对汉字进⾏标注训练，不仅考虑了词语出现的频率，还考虑上下⽂，具备较好的学习能⼒，因此其对歧义词和未登录词的识别都具有良好的效果

- 优点: 适应性较强
- 缺点：成本较⾼，速度较慢

3、基于深度学习

例如有⼈员尝试使⽤双向LSTM+CRF实现分词器，其本质上是序列标注，所以有通⽤性，命名实体识别等都可以使⽤该模型，据报道其分词器字符准确率可⾼达97.5%

- 优点： 准确率⾼、适应性强
- 缺点：成本⾼，速度慢

**常⻅的分词器都是使⽤机器学习算法和词典相结合，⼀⽅⾯能够提⾼分词准确率，另⼀⽅⾯能够改善领域适应性**

## 词干提取、词形还原

### **词⼲提取 - Stemming**

词⼲提取是去除单词的前后缀得到词根的过程，⼤家常⻅的前后词缀有「名词的复数」、「进⾏式」、「过去分词」…

### **词形还原 - Lemmatisation**

词形还原是基于词典，将单词的复杂形态转变成最基础的形态。词形还原不是简单地将前后缀去掉，⽽是会根据词典将单词进⾏转换。⽐如「drove」会转换为「drive」。

**为什么要做词⼲提取和词形还原？**

⽐如当我搜索「play basketball」时，Bob is playing basketball 也符合我的要求，，
但是 play 和 playing 对于计算机来说是 2 种完全不同的东⻄，所以我们需要将 playing
转换成 play。
词⼲提取和词形还原的⽬的就是将⻓相不同，但是含义相同的词统⼀起来，这样⽅便后续
的处理和分析。

**词⼲提取和词形还原的 4 个相似点**

1. ⽬标⼀致。词⼲提取和词形还原的⽬标均为将词的屈折形态或派⽣形态简化或归并
为词⼲（stem）或原形的基础形式，都是⼀种对词的不同形态的统⼀归并的过程。
2. 结果部分交叉。词⼲提取和词形还原不是互斥关系，其结果是有部分交叉的。⼀部
分词利⽤这两类⽅法都能达到相同的词形转换效果。如“dogs”的词⼲为“dog”，其
原形也为“dog”。
3. 主流实现⽅法类似。⽬前实现词⼲提取和词形还原的主流实现⽅法均是利⽤语⾔中
存在的规则或利⽤词典映射提取词⼲或获得词的原形。
4. 应⽤领域相似。主要应⽤于信息检索和⽂本、⾃然语⾔处理等⽅⾯，⼆者均是这些
应⽤的基本步骤。

**词⼲提取和词形还原的 5 个不同点**

1. 在原理上，词⼲提取主要是采⽤“缩减”的⽅法，将词转换为词⼲，如将“cats”处理
为“cat”，将“effective”处理为“effect”。⽽词形还原主要采⽤“转变”的⽅法，将
词转变为其原形，如将“drove”处理为“drive”，将“driving”处理为“drive”。
2. 在复杂性上，词⼲提取⽅法相对简单，词形还原则需要返回词的原形，需要对词形
进⾏分析，不仅要进⾏词缀的转化，还要进⾏词性识别，区分相同词形但原形不同
的词的差别。词性标注的准确率也直接影响词形还原的准确率，因此，词形还原更
为复杂。
3. 在实现⽅法上，虽然词⼲提取和词形还原实现的主流⽅法类似，但⼆者在具体实现
上各有侧重。词⼲提取的实现⽅法主要利⽤规则变化进⾏词缀的去除和缩减，从⽽
达到词的简化效果。词形还原则相对较复杂，有复杂的形态变化，单纯依据规则⽆
法很好地完成。其更依赖于词典，进⾏词形变化和原形的映射，⽣成词典中的有效
词。
4. 在结果上，词⼲提取和词形还原也有部分区别。词⼲提取的结果可能并不是完整
的、具有意义的词，⽽只是词的⼀部分，如 “revival” 词⼲提取的结果
为“reviv”，“ailiner”词⼲提取的结果为“airlin”。⽽经词形还原处理后获得的结果是
具有⼀定意义的、完整的词，⼀般为词典中的有效词。
5. 在应⽤领域上，同样各有侧重。虽然⼆者均被应⽤于信息检索和⽂本处理中，但侧
重不同。词⼲提取更多被应⽤于信息检索领域，如Solr、Lucene等，⽤于扩展检
索，粒度较粗。词形还原更主要被应⽤于⽂本挖掘、⾃然语⾔处理，⽤于更细粒
度、更为准确的⽂本分析和表达

## 词性标注

**什么是词性标注？**

词性指以词的特点作为划分词类的根据。词类是⼀个语⾔学术语，是⼀种语⾔中词的语法分类，是以语法特征（包括句法功能和形态变化）为主要依据、兼顾词汇意义对词进⾏划分的结果。

从组合和聚合关系来说，⼀个词类是指：在⼀个语⾔中，众多具有相同句法功能、能在同样的组合位置中出现的词，聚合在⼀起形成的范畴。词类是最普遍的语法的聚合。词类划分具有层次性。如汉语中，词可以分成实词和虚词，实词中⼜包括体词、谓词等，体词中⼜可以分出名词和代词等。

词性标注就是在给定句⼦中判定每个词的语法范畴，确定其词性并加以标注的过程，这也是⾃然语⾔处理中⼀项⾮常重要的基础性⼯作，所有对于词性标注的研究已经有较⻓的时间，在研究者⻓期的研究总结中，发现汉语词性标注中⾯临了许多棘⼿的问题。

**中文词性标注的难点**

汉语是⼀种缺乏词形态变化的语⾔，词的类别不能像印欧语那样，直接从词的形态变化上来判别.

常⽤词兼类现象严重。《现代汉语⼋百词》收取的常⽤词中，兼类词所占的⽐例⾼达
22.5%，⽽且发现越是常⽤的词，不同的⽤法越多。由于兼类使⽤程度⾼，兼类现象涉及
汉语中⼤部分词类，因⽽造成在汉语⽂本中词类歧义排除的任务量巨⼤。

研究者主观原因造成的困难。语⾔学界在词性划分的⽬的、标准等问题上还存在分歧。⽬
前还没有⼀个统的被⼴泛认可汉语词类划分标准，词类划分的粒度和标记符号都不统⼀。
词类划分标准和标记符号集的差异，以及分词规范的含混性，给中⽂信息处理带来了极⼤
的困难

**词性标注的4种常见方法**

1、基于规则的词性标注⽅法:   

基于规则的词性标注⽅法是⼈们提出较早的⼀种词性标注⽅法，其基本思想是按兼类词搭
配关系和上下⽂语境建造词类消歧规则。早期的词类标注规则⼀般由⼈⼯构建。
随着标注语料库规模的增⼤，可利⽤的资源也变得越来越多，这时候以⼈⼯提取规则的⽅
法显然变得不现实，于是乎，⼈们提出了基于机器学习的规则⾃动提出⽅法

2、基于统计模型的词性标注方法

统计⽅法将词性标注看作是⼀个序列标注问题。其基本思想是：给定带有各⾃标注的词的
序列，我们可以确定下⼀个词最可能的词性。
现在已经有隐⻢尔可夫模型（HMM）、条件随机域（CRF）等统计模型了，这些模型可
以使⽤有标记数据的⼤型语料库进⾏训练，⽽有标记的数据则是指其中每⼀个词都分配了
正确的词性标注的⽂本

3、基于统计方法与规则方法相结合的词性标注方法

理性主义⽅法与经验主义相结合的处理策略⼀直是⾃然语⾔处理领域的专家们不断研究和
探索的问题，对于词性标注问题当然也不例外。
这类⽅法的主要特点在于对统计标注结果的筛选，只对那些被认为可疑的标注结果，才采
⽤规则⽅法进⾏歧义消解，⽽不是对所有情况都既使⽤统计⽅法⼜使⽤规则⽅法

4、基于深度学习的词性标注方法

可以当作序列标注的任务来做，⽬前深度学习解决序列标注任务常⽤⽅法包括
LSTM+CRF、BiLSTM+CRF等。
值得⼀提的是，这⼀类⽅法近年来⽂章⾮常多，想深⼊了解这⼀块的朋友们可以看这⾥：
[NLP-progress – GitHub](https://github.com/sebastianruder/NLP-progress/blob/master/english/part-of-speech_tagging.md)

## 命名实体识别NER

命名实体识别（Named Entity Recognition，简称NER），⼜称作“专名识别”，是指识
别⽂本中具有特定意义的实体，主要包括⼈名、地名、机构名、专有名词等。简单的讲，
就是识别⾃然⽂本中的实体指称的边界和类别

### NER的实现方式

**有监督的学习⽅法**：这⼀类⽅法需要利⽤⼤规模的已标注语料对模型进⾏参数训练。⽬前

常⽤的模型或⽅法包括隐⻢尔可夫模型、语⾔模型、最⼤熵模型、⽀持向量机、决策树和

条件随机场等。值得⼀提的是，基于条件随机场的⽅法是命名实体识别中最成功的⽅法。

**半监督的学习⽅法**：这⼀类⽅法利⽤标注的⼩数据集（种⼦数据）⾃举学习。

**⽆监督的学习⽅法**：这⼀类⽅法利⽤词汇资源（如WordNet）等进⾏上下⽂聚类。

**混合⽅法**：⼏种模型相结合或利⽤统计⽅法和⼈⼯总结的知识库。