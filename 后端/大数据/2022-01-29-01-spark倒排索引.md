---
slug: spark倒排索引
title: spark倒排索引
author: Jessie
author_title: 后端开发
author_url: https://github.com/OnfireMrHuang
author_image_url: https://avatars.githubusercontent.com/u/22336719?v=4
tags: [Golang, Rust, kubernetes, spark, Java]
---

## 目的

倒排索引的意思就是原来是通过文件路径、词下标查找出单词，现在则通过单词反查出文件路径和单词下标。

## 版本记录

本地JAVA版本:  java version "1.8.0_301"
Scala版本:  Scala code runner version 2.13.8
Hadoop版本: hadoop-3.2.2
Spark版本: spark-3.2.0

## 伪代码

```text

1、从输入文件目录获取文件列表

2、读取文件内容并split出单词，并构建映射{{file_name,word}: 1}列表

3、union合并每个文件的映射{{file_name,word}: 1}组成大列表

4、根据key {file_name,word} 聚合word的count，构建为{{file_name,word}: count}列表

5、将{{file_name,word}: count} map转换为{word: (file_name,count)}

6、根据键word聚合{word: (file_name,count)}

```

## 具体代码以及注释

```scala

package org.example

import org.apache.spark.{SparkContext,SparkConf}
import org.apache.hadoop.fs.{FileSystem, Path}
import scala.collection.mutable.ArrayBuffer

object InvertIndex extends App {

	// 设置spark context
	val sparkConf = new SparkConf().setMaster("local").setAppName(this.getClass.getName)
	val sc = new SparkContext(sparkConf)
	sc.setLogLevel("WARN")

	val input = "/tmp/test/spark"
	val fs = FileSystem.get(sc.hadoopConfiguration)
	val fileList = fs.listFiles(new Path(input), true)

	// 构建一个空RDD
	var rdd = sc.emptyRDD[(String, String)] // filename, word
	while(fileList.hasNext) {
		val path = fileList.next
		val fileName = path.getPath.getName
		// 获取文件内容并split单词组成文件名、单词的映射，最后union合并为一个RDD
		rdd = rdd.union(sc.textFile(path.getPath.toString)
		.flatMap(_.split("\\s+")).map((fileName,_)))
	}

	//  println("---"*10)
	//  rdd.foreach(println)
	//  println("---"*10)

	// 给每个单词初始化为{file_name,word},1，并根据{file_name,word}合并sum
	val rdd2 = rdd.map((_,1)).reduceByKey(_+_)

	//  println("---"*10)
	//  rdd2.foreach(println)
	//  println("---"*10)

	// 设置map为word:({file_name},{count}),最后再根据word合并字符串，用,隔开，最后map格式化打印
	val rdd3 = rdd2.map(data => (data._1._2,
		String.format("(%s,%s)",data._1._1, data._2.toString)))
		.reduceByKey(_ + "," + _)
		.map(data => String.format("%s, {%s}",data._1,data._2))

  println("---"*10)
  rdd3.foreach(println)
  println("---"*10)
}
```
