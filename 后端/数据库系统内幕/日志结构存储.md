# 日志结构存储

## LSM树 — (Log-Structured Merge Tree)

讨论B树时，我们得出结论： 使用缓存可以改善空间开销和写入放大问题。通常不同的存储结构有两种应用缓存的方式: 推迟对磁盘驻留页的写入传播、以及是写操作顺序化。

### **LSM树特性**

1. LSM树是最流行的不可变磁盘存储结构之一，它使用缓存和仅追加存储来实现顺序写操作。LSM树写入不可变的文件，并随着时间的推移将它们合并在一起。这些文件通常包含它们自己的索引，以帮助读取者高效地定位数据。
2. LSM树中的合并一词表示： 由于它的不变性，需要使用类似于归并排序的方法来合并树的内容。这一过程会发生在回收冗余副本所占空间的维护期，也会发生在向用户返回内容之前的读取期。
3. LSM 树将数据文件写人推迟，并将更改缓冲在一个内存驻留表中。表中的内容随后将被写到不可变磁盘文件中。在文件完全持久化之前，所有数据记录仍然可以在内存中访问。
4. 保持数据文件的不变性有利于顺序写入：数据一次性写入磁盘，文件是仅追加的。可变结构也可以一次性预分配数据块（例如，索引顺序访问方法 (ISAM [RAMAKRISHNAN03,LARS N81]，但是后续访问仍然需要随机读写。而不可变结构则允许我们按顺序放置数据以防止产生碎片。此外，不可变文件具有更高的数据密度：我们无须为之后将要写入的数据记录保留任何额外的空间，也不会因更新后的记录可能比最初写入的记录需要更多空间而为其保留额外的空间。
5. 由于文件是不可变的，所以插人、更新和删除操作不需要在磁盘上定位数据记录，这显著提高了写入的性能和吞吐量。另一方面，重复内容是允许的，且要在读取时解决冲突。 LS M树在写远大于读的应用程序中特别有用。考虑到数据量和采集速率在不断增长，在现代数据密集型系统中这种情况是常见的。
6. 读和写在设计上并不交叉，因此可以在没有段级锁的情况下读写磁盘上的数据，这大大简化了并发访问。相反，可变结构采用层级锁和闩锁来确保磁盘数据结构的完整性并允许并发读取，但要求写入者对子树具有独占的所有权。基于 LSM 的存储引擎使用数据和索引文件的线性化内存视图，并且只需对管理它们的结构进行并发访问保护。
7. B树和 LSM 树都需要一些内务处理 (housekeeping) 来优化性能，但原因不尽相同。由于分配文件的数量稳定增长，所以 LSM 树必须合并和重写文件，以确保在读取过程中访问尽可能少的文件，因为请求的数据记录可能分布在多个文件中。另一方面，可变文件可能必须被部分或全部重写，以减少碎片并回收被更新或删除的记录所占用的空间。当然，内务处理的确切工作很大程度上取决于具体实现。

### L SM树的结构

LSM树由较小的内存驻留组件和较大的磁盘驻留组件组成。要在磁盘上写出不可变的文件内容，首先需要在内存中进行缓冲和排序。

内存驻留组件(memtable)是可变的: 它缓冲数据记录，并充当读写操作的目标。当其大小达到一个可配阈值时， memtable 中的内容将会被持久化到磁盘上。memtable 的更新不需要磁盘访问，也没有相关的IO开销。需要一个单独的预写日志文件来保证数据记录的持久性；在向客户端确认操作之前，数据记录会被追加到日志中并提交到内存。

磁盘驻留组件则是通过将内存中缓冲的内容刷写到磁盘来构建的。磁盘驻留组件仅用于读取： 缓存的内容被持久化成文件，而这些文件永远不会被修改。这允许我们从简单操作的角度来考虑: 对内存中的表进行写操作，以及对磁盘和基于内存的表进行读、合并和文件删除操作。

**双组件LSM树**

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled.png)

在刷写子树之后，原来的内存驻留子树和磁盘驻留子树将被丢弃，并被它们合并后的结果所替换。在替换后，该结果就可以从磁盘驻留树中先前存在的部分被寻址到。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%201.png)

在实现子树合并和刷写时，我们必须确保三件事:

1. 刷新过程一旦开始，所有新的写操作都必须转到新的memtable
2. 在子树刷写期间，磁盘驻留子树和正在刷写的内存驻留子树都要保持可以被读取的状态
3. 在刷写之后，如下操作必须以原子的方式执行: 发布合并的内容、丢弃原始的磁盘驻留

**多组件LSM树**

多组件LSM树具有不止一个磁盘驻留表，在这种情况下，整个memtable内容在一次运行中被刷写。而在多次刷写之后将会得到多个磁盘驻留表，而且它们的数量只会随着时间的推移而增长。由于我们并不总是确切地知道哪些表持有我们所需的数据记录，所以我们可能不得不访问多个文件来定位要搜索的数据。

从多个数据源而不是仅从一个数据源读取数据的代价较大。为了缓解这个问题并将表的数量维持在最少，摇要触发一个称为压实(cmpaction) 的周期性合并过程。压实选择几个表，读取它们的内容，合并它们，井将合井的结果写到新文件中。旧表在新表出现的同时袚丢弃.

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%202.png)

**内存表**

刷写 memtable 可以被周期性地触发，也可以通过大小阈值来触发。 在其可被刷写之前，必须进行 memtable 切换：分配一个新的 memtable, 它成为所有新的写人操作的目标，而旧的 memtable 则变成刷写状态。这两个步骤必须被原子地执行。在其内容被完全刷写之前，被刷写的 memtable 仍可用于读取。在此之后，旧的 memtable 将被丢弃，取而代之的是 个新写入的磁盘驻留表，该表可用于读取。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%203.png)

数据已经在内存中进行了排序，因此可以通过将内存驻留的内容依次写入磁盘来创建磁盘驻留表。在刷写期间，正在刷写的 memtable 和当前的 memtable 都可供读取。在完全刷写 memtable 之前，其内容唯一的磁盘驻留版本储存在预写日志中。当memtable 内容被完全刷写到在磁盘上时，日志可以被修剪 (trim) ，并且保存对被刷写过的memtable 进行的操作的日志段可以被丢弃。

### 更新与删除

在LSM 树中，插入、更新和删除操作不需要在磁盘上定位数据记录。但是，在读取的过程中需要对冗余的记录进行协调。

从memtable 中删除数据记录是不够的，因为其他磁盘或内存驻留表可能持有同一个键的数据记录。如果仅通过从 memtable 中移除数据项来实现删除，那么要么最终删除无效，要么我们会复活 (resurrect) 先前的数值。

因此，删除操作需要被显式地记录下来。可以通过一个特殊的删除条目，有时称为墓碑(tombstore)或休眠凭证(dormant certificate)来实现。而对于删除连续范围的键则可以通过谓词删除来完成，其工作方式是在删除条目中附加一个谓词，该谓词根据常规的记录排序规则进行排序。在协调期间，与谓词匹配的数据记录将被跳过而不会返回客户端。

### LSM树的查找

LSM 树由多个组件组成。查找过程通常会访问多个组件，因此在将其返回客户端之前，必须对其内容进行合并和协调。

**1、合并迭代**

由于磁盘驻留表上的内容是经过排序的．所以我们可以使用多路归并排序算法。例如，我们有三个数据源：两个磁盘驻留表和 memtable 。通常，存储引擎提供游标或迭代器来遍历文件内容。

多路归井排序使用一个优先级队列，如最小堆 (min-heap) [SEDGEWICK 11] ，该队列最多保存N个元素（其中N是迭代器的数目），该队列对其内容进行排序井准备返回的下一个最小的元素。每个迭代器的头被放入队列。队列头部的元素就是所有迭代器的最小值。

当从队列中移除最小的元素时，检查与之相关联的迭代器是否有下 个值，然后将该值放入队列，队列将被重新排序以维持顺序。

因为所有迭代器中的内容都是有序的，所以从之前的最小元素对应的迭代器取出下一个值并插入优先队列也维持了 个不变式一一队列仍然包含来自所有迭代器的最小元素。当其中一个迭代器耗尽时，就不再插入这个迭代器的值， 但是算法仍然继续执行。该算法持续执行到查询条件被满足或所有迭代器耗尽为止。

在合并迭代过程中，可能会有多个数据记录具有相同的键。从优先级队列和迭代器不变式中可知，如果在每个迭代器中每个键只持有单个数据记录，而我们又最终在队列中发现多个数据记录且有相同的键，那么这些数据记录一定来自不同的迭代器。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%204.png)

**2、协调**

合并迭代只是从多个数据源合并数据的一个方面。另一个重要方面是与同一键相关联的数据记录的协调和冲突解决。

为了协调数据记录，我们需要了解它们中的哪一个是优先的 。数据记录持有这样做所必需的元数据，例如时间戳。我们可以通过比较时间戳来建立来自多个数据源的数据项之间的顺序，并找出哪个更新。
被具有更高时间戳的记录所遮蔽的记录不会返回给客户端、在压实期间也不会被写人。

**3、LSM树的维护**

在B树中，维护过程收集未引用的单元格，对页进行碎片整理，并回收被移除和遮蔽的记录所占用的空间。在 LSM 树中，磁盘驻留表的数量不断增长、但可以通过触发周期性的压实来减少。

压实会挑选多个磁盘驻留表，使用之前提到的合并和协调算法来迭代它们的全部内容，并将结果写入新创建的表。在压实过程完成之前，正在被压实的表仍可用于读取，这意味着在压实期间，磁盘上需要有足够的可用空间来写入压实的表。

**分层压实**

压实提供了多种优化机会，井且有许多不同的压实策略。其中一种常用的压实策略称为层级压实 (leveled compaction) ，例如 RocksDB 就使用这一策略。

- 0层表是通过刷写memtable的内容而创建的。0层中的表可能包含重复的键范围。 0层上的表数量只要达到某个阈值，它们的内容就会被合并，并创建一个层级为1的新表
- 1层和所有序号更高的层上的表都不存在键范围上的重叠，因此在压实期间必须将0层表分区，分裂成多个范围，然后将其与持有相应键范围的表进行合并。或者，压实可以读入所有0层和1层的表，然后输出分区过的1层的表。
- 在序号更高的层上进行压实是从两个范围重叠的连续层中挑选表，并在较高的层上生成一个新的表。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%205.png)

层的大小在层与层之间是指数增长的：每一个层上的表都要比上一层上的表以指数级增大。这样，最新的数据总是在序号最低的层上，而较旧的数据逐渐迁移到序号较高的层上。

**按大小分层压实**

另一种流行的压实策略称为按大小分层压实 (size-tiered compaction) 。在按大小分层压实中，磁盘驻留表不是根据其层级进行分组，而是按照大小进行分组：较小的表与较小的表分在一起，较大的表则与较大的表分在一起。

## 读写放大与空间放大

一种方法是回收重复记录占用的空间，减少空间开销，但这会产生由不断重写表导致的更高的写放大。替代方案是避免连续重写数据，而这又增加了读放大（在读取期间协调关联到相同键的数据记录的开销）和空间放大（因为冗余记录会袚保存更长时间。

有一种流行的存储结构开销模型考虑了如下 个因素：读取 (Read) 、更新 (Update)、内存 (Memory) 开销。它被称为 RUM 猜想。

RUM 猜想指出，减少其中两项开销将不可避免地导致第三项开销的恶化，井且优化只能以牺牲三个参数中的一个为代价。我们可以根据这三个参数对不同的存储引擎进行比较、以了解它们针对哪些参数进行了优化，以及其中隐含着哪些可能的权衡。

- B树是针对读进行优化的。对B树的写入需要在磁盘上找到一个记录，而随后对同一页的写人可能需要多次更新该磁盘页面。还要为将来的更新和删除保留额外的空间，这增加空间开销。
- LSM 树则不需要在写入期间定位磁盘上的记录，也不用为以后的写入保留额外的空间。但由于存储冗余记录，所以仍然存在一些空间开销。在默认配置中，由于必须通过访问多个表才能返回完整的结果，所以读取的成本更高。

RUM猜想开销模型并不完美，因为它没有考虑其他重要的指标: 延迟、访问模式、实现复杂度、维护成本以及与硬件相关的细节。对于分布式数据库很重要的概念，如一致性含义和复制开销，也没有被考虑进去。

## LSM树实现细节

### 有序字符串表

磁盘驻留表通常使用有序字符串表 (Sorted String Table, SSTable) 来实现。顾名思义，SSTable 中的数据记录是按照键顺序进行排序和布局的。 SSTable 通常由两个组件组成：索引文件和数据文件。索引文件是用能够以对数时间复杂度（如B树)或常量时间复杂度（如哈希表）进行查找的某种结构来实现的。

由于数据文件以键顺序保存记录，所以使用哈希表进行索引并不妨碍我们实现范围扫描，因为访问哈希表只是为了定位范围中的第一个键，并且在范围谓词仍然匹配的情况下，范围本身可以顺序地从数据文件中读出。

### 布隆过滤器

LSM 树中读放大的来源是，我们必须寻址多个磁盘驻留表，以便完成读取操作。这是因为我们不一定能预先知道一个磁盘驻留表是否包含要搜索的键指向的数据记录。

防止表查询的方法之一是在元数据中存储其键的范围（存储给定表中的最小和最大键），并检查要搜索的键是否在该表的范围之内。这一信息是不精确的，它只能告诉我们数据记录是否可能会出现在表中。为了改进这种情况，则使用了布隆过滤器的数据结构。

我们可以使用布隆过滤器来判断键是否可能在表中或肯定不在表中。在查询期间跳过布隆过滤器返回“不匹配＂的文件，而只访问其余文件，以查明数据记录是否确实存在。使用与磁盘驻留表相关联的布隆过滤器能显著减少读取过程中要访问的表的数量。

### 跳表

跳表由一系列高度不同的节点组成，构建了链接在一起的层次结构．允许跳过数据项的范围。每个节点拥有一个键，并且与链表中的节点不同，有些节点不止有一个后继节点。高度为h的节点与高度小于等于h的一个或多个前导节点所链接。最低层上的节点可以与任意高度的节点所链接。(详细算法不在展开，网上一搜一大把)

Apache Cassandra 将跳表用于二级索引memtable实现，来加速单表查询。

### 磁盘访问

由于大多数表的内容都驻留在磁盘上，并且存储设备通常允许按块访问数据，所以许多LSM 树的实现依赖于页缓存，并通常将其用于磁盘访问和中间缓存。

### 压缩

当按页压缩数据时，压缩后的页不是页对齐的，因为它们的大小比未压缩的小。

为了能够处理被压缩的页，我们需要在写入它们的内容时跟踪它们的地址边界。我们可以用零来填充压缩的页，使它们与页大小对齐，但这样我们就失去了压缩的好处。

为了使被压缩的页可寻址，我们需要一个间接层来存储偏移量和压缩页的大小 。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%206.png)

在压缩和刷盘期间，压缩的页被顺序地追加 ，并且压缩信息（原始未压缩的页偏移量和实际压缩的页偏移量）存储在单独的文件段 。在读取过程中，将压缩后的页偏移量及其大小查询出来，之后在 内存中解压缩并物化

## 无序LSM存储

无序存储一般不需要单独的日志，井且允许我们按插入顺序存储数据记录以减少写入的开销。

### 1、Bitcask

Bitcask是Riak中使用的存储引擎之一 ，它不使用memtable进行缓冲，而是将数据记录直接存储在日志文件中。

为了使值可被搜索， Bitcask使用一种名为 keydir （键目录）的数据结构，它保存与键相应的最新数据记录的引用。旧的数据记录可能仍然在磁盘中，但不会被 keydir 引用，并且它们会在压实过程中被垃圾收集。 keydir 被实现为内存中的哈希表，并且在启动期间必须从日志文件中重建。

- 在一次写入中，键和数据记录被顺序地追加到日志文件中，并且会在 keydir 中放入指向新写入的数据记录位置的指针。
- 读取时会查看 keydir 以定位要搜索的键，并跟随指向对应日志文件的指针以定位数据记录。由于在任何给定时刻都只能有一个值与 keydir 中的键相关联，所以点查询不必合并来自多个数据源的数据。
- 在压实过程中，所有日志文件的内容被顺序读取 、合并、写入到新的位置，只保留活动
数据记录，丢弃被遮蔽的数据。使用指向移动过的数据记录的新指针来更新 keydir

数据记录直接存储在日志文件中，因此不必维护单独的预写日志，这减少了空间开销和写放大。这种方法的缺点是它只支持单点查询，不允许范围扫描，因为数据项在 keydir和数据文件中都是无序的。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%207.png)

### 2、WiscKey

WiscKey[LU16] 通过在LSM 中保存键排序并在称为 vLog （值日志） 的无序仅追加文件中保存数据记录，来将排序与垃圾收集解耦。这种方法可以解决讨论 Bitcask 时提到的两个问题：需要将所有键保留在内存中，以及需要在启动时重新构建哈希表。

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%208.png)

这里的主要挑战是， 由于 vLog 数据是未排序的，所以范围扫描需要随机IO。 WiscKey在范围扫描时使用固态硬盘的内部并行性来并行预取数据块，以减少随机 l/0 的开销。在数据块传输方面，其成本仍然很高 ：在范围扫描期间，要获取一条的数据记录，必须读取该数据记录所在的整个页。

由于vLog 中的数据未排序，并且不包含存活信息，所以必须扫描键树以查找哪些值仍然是存活的。在垃圾收集期间执行这些检查引入了额外的复杂度。传统的LSM 树可以在压实期间直接解析文件内容，而无须处理键索引。

## LSM树中的并发

LSM树中的并发挑战主要与切换表视图(在刷写和压缩过程中更改的内存驻留表和磁盘驻留表的集合)和日志同步有关。memtable通常也是并发访问的。

在刷写期间，必须遵循以下规则:

- 新的memtable必须对读写可用
- 旧的memtable必须对读保持可见。
- 正在刷写的memtable必须写到磁盘上
- 丢弃已经刷写的memtable与创建刷写磁盘驻留表这两个操作必须被原子地执行
- 预写日志中，记录之前曾应用于被刷写memtable的操作的日志段必须被丢弃

例如，Apache Cassandra通过使用操作顺序屏障来解决这些问题:  在memtable刷写之前，所有已经接受的写操作都必须等待。这样，刷写进程就知道哪些其他进程依赖于它。

更一般地，我们有以下同步点:

- memtable切换 — 在此之后，所有的写操作都去到新的memtable,使其成为主memtable，而旧的memtable仍可用于读操作。
- 刷写完成 — 在表视图中用刷写完的磁盘驻留表来替换旧的memtable
- 预写日志截断 — 丢弃持有与被刷写memtable相关联的记录的日志段。

## 日志堆叠

许多现代文件系统都使用日志结构: 它们在内存段中缓冲写操作，并在缓冲满的时候以仅追加方式将内容刷写到磁盘上。固态硬盘也使用日志结构存储，以处理小的随机写入、最小化写入开销、均衡磨损以及延长设备寿命。

随着固态硬盘变得更经济，日志结构存储 (Log-Structured Storage, LSS) 系统开始流行LSM 树和固态硬盘是个很好的搭配，因为序列化的工作负载和仅追加写入有助于减少原地更新带来的放大，而原地更新会对固态硬盘的性能产生负面影响。

**1、闪存转换层**

在固态硬盘中使用日志结构映射层是由两个因素驱动的： 

1. 小型随机写入必须在物理页中一起被批处理
2. 固态硬盘是通过使用编程/擦除周期 (program/erase cycle)来工作的

**2、文件系统日志记录**

![Untitled](%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E5%AD%98%E5%82%A8%20fe2b10ba07334b9d8e65023e56156a8b/Untitled%209.png)

未对齐的段写入可能会使情况变得更精，因为丢弃较高层的日志段可能会导致相邻段部分的碎片化和移动

## 小结

日志结构存储 (LSS) 使用广泛：从闪存转换层到文件系统和数据库系统。在内存中，将小的随机写人放在一起进行批量处理有助于减少写放大。为了回收被移除段所占用的空间， LSS 定期触发垃圾收集。

LSM 树借鉴了 LSS 的一些思想，构建以日志结构方式管理的索引结构：写入在内存中成批处理并刷写到磁盘；被遮蔽数据记录在压实过程中被清理。

重要的是要记住许多软件层都在使用 LSS, 并确保层的堆叠是最佳的。或者，我们可以完全绕过文件系统层，直接访问硬件。